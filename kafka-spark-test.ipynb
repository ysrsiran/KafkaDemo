{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Liberaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from kafka import KafkaProducer\n",
    "from kafka import KafkaConsumer\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(filepath):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root,'*.json'))\n",
    "        for f in files :\n",
    "            all_files.append(os.path.abspath(f))\n",
    "    \n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get file path and set up producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files('meta_data')\n",
    "producer = KafkaProducer(bootstrap_servers =\n",
    "  ['localhost:9092'],value_serializer=lambda v: json.dumps(v).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce message and publish to Kafka topic (input_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        producer.send('input_topic', value = data, key =json.dumps(data['userId']).encode('utf-8'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up enviroment and spark, using spark dataframe to load input_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 pyspark-shell'\n",
    "\n",
    "sc = SparkSession.builder.appName('Pyspark_kafka_read_write').getOrCreate()\n",
    "\n",
    "df = sc \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"input_topic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load() \\\n",
    "    .select(\"value\") \\\n",
    "    .selectExpr(\"CAST(value AS STRING) as json\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField(\"userId\",StringType(),True), \\\n",
    "    StructField(\"type\",StringType(),True), \\\n",
    "    StructField(\"metadata\",StructType([\\\n",
    "                    StructField('messageId', StringType(), True), \\\n",
    "                    StructField('sentAt', LongType(), True), \\\n",
    "                    StructField('timestamp', LongType(), True), \\\n",
    "                    StructField('receivedAt', LongType(), True), \\\n",
    "                    StructField('apiKey', StringType(), True), \\\n",
    "                    StructField('spaceId', StringType(), True), \\\n",
    "                    StructField('version', StringType(), True), \\\n",
    "                    \n",
    "                    ])),\\\n",
    "    StructField(\"event\", StringType(), True), \\\n",
    "    StructField(\"eventData\", StructType([\n",
    "            StructField('MovieID', StringType(), True)\\\n",
    "])) \\\n",
    "  ])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing and selecting the right column data\n",
    "df = df.withColumn(\"jsonData\", from_json(col(\"json\"), schema)) \\\n",
    "                .select(\"jsonData.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- metadata: struct (nullable = true)\n",
      " |    |-- messageId: string (nullable = true)\n",
      " |    |-- sentAt: long (nullable = true)\n",
      " |    |-- timestamp: long (nullable = true)\n",
      " |    |-- receivedAt: long (nullable = true)\n",
      " |    |-- apiKey: string (nullable = true)\n",
      " |    |-- spaceId: string (nullable = true)\n",
      " |    |-- version: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      " |-- eventData: struct (nullable = true)\n",
      " |    |-- MovieID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter and aggregate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info = df.select('userId','metadata.sentAt','metadata.receivedAt')\n",
    "user_info = user_info.withColumn(\"seenTime\",col('sentAt')+col('receivedAt'))\n",
    "result = user_info.groupby(\"userId\").agg(min('seenTime').alias('firstSeen'),max('seenTime').alias('lastSeen'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write output topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = result.selectExpr(\"CAST(userId AS STRING) AS key\", \"to_json(struct(*)) AS value\")\\\n",
    "                .write \\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "                .option(\"topic\", \"output_topic\") \\\n",
    "                .option(\"checkpointLocation\", \"./check\") \\\n",
    "                .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer ('output_topic',bootstrap_servers = ['localhost:9092'],\n",
    "value_deserializer=lambda m: json.loads(m.decode('utf-8')),consumer_timeout_ms=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
